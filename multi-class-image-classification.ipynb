{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multiclass Classification","metadata":{}},{"cell_type":"markdown","source":"## Features\n\n1. Done by Gokulakannan S\n2. Multi class image classification Task\n3. Model Used: VGG Model\n4. Data present in Kaggle : https://www.kaggle.com/nadeemsk4347/classifying-flowers-with-keras/data . It is the same data provided to me by TMLC.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-26T12:30:09.849503Z","iopub.execute_input":"2021-10-26T12:30:09.850280Z","iopub.status.idle":"2021-10-26T12:30:09.887871Z","shell.execute_reply.started":"2021-10-26T12:30:09.850183Z","shell.execute_reply":"2021-10-26T12:30:09.887071Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Importing the necessary Libraries\nimport os\nimport numpy as np\nimport scipy.io\nimport keras\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport tarfile\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import confusion_matrix\nfrom keras import Sequential\nfrom tensorflow.keras.applications import VGG19 #For Transfer Learning\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import SGD,Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:30:11.119359Z","iopub.execute_input":"2021-10-26T12:30:11.120053Z","iopub.status.idle":"2021-10-26T12:30:17.143310Z","shell.execute_reply.started":"2021-10-26T12:30:11.120014Z","shell.execute_reply":"2021-10-26T12:30:17.142544Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Step 1 : Processing Labels","metadata":{}},{"cell_type":"code","source":"img_labels = scipy.io.loadmat(\"/kaggle/input/flower-dataset-102/imagelabels.mat\")\nimg_labels = img_labels[\"labels\"]\nimg_labels = img_labels[0]\nfor i in range(len(img_labels)):\n  img_labels[i] = img_labels[i] - 1\nprint(img_labels)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-10-26T12:30:17.144873Z","iopub.execute_input":"2021-10-26T12:30:17.145151Z","iopub.status.idle":"2021-10-26T12:30:17.195195Z","shell.execute_reply.started":"2021-10-26T12:30:17.145112Z","shell.execute_reply":"2021-10-26T12:30:17.194457Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Step 2 : Processing Images","metadata":{}},{"cell_type":"code","source":"# We are reducing the dimensions of the images to 50 X 50\nIMG_SIZE = 50\n\ntrain_x = []\ntrain_y = []\ntar = tarfile.open('/kaggle/input/flower-dataset-102/102flowers.tgz', \"r:gz\")\ni = 0\nfor tarinfo in tqdm(tar):\n    i+=1\n    tar.extract(tarinfo.name)\n    \n    if(tarinfo.name[-4:] == '.jpg'):\n        var = tarinfo.name[11:15]\n        img_num = int(var)-1\n        train_y.append(img_labels[img_num])\n        \n        image = cv2.imread(tarinfo.name) #reading the image\n        resized = cv2.resize(image, (IMG_SIZE,IMG_SIZE)) # changing image size to 50 X 50\n        normalized_img = cv2.normalize(resized, None, alpha=0, beta=1, \n                                norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F) #normalizing the pixels\n        train_x.append(normalized_img)\n\n#         label_list.append(tarinfo.name.split('_')[0])\n    if(tarinfo.isdir()):\n        os.rmdir(tarinfo.name)\n    else:\n        os.remove(tarinfo.name) \n\ntar.close()\ntrain_x = np.array(train_x) # creating the training data","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:30:18.068309Z","iopub.execute_input":"2021-10-26T12:30:18.068863Z","iopub.status.idle":"2021-10-26T12:31:06.400995Z","shell.execute_reply.started":"2021-10-26T12:30:18.068822Z","shell.execute_reply":"2021-10-26T12:31:06.400069Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Change the number in from 0 to 8189 to look into any image\nplt.imshow(train_x[8180])\nprint(\"Image shape\",train_x.shape)\nprint(\"The Label of this image is \",train_y[8180])","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:32:49.959489Z","iopub.execute_input":"2021-10-26T12:32:49.959766Z","iopub.status.idle":"2021-10-26T12:32:50.170510Z","shell.execute_reply.started":"2021-10-26T12:32:49.959735Z","shell.execute_reply":"2021-10-26T12:32:50.169091Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Step 3 : Data Validation (Train, Validation and Test split)","metadata":{}},{"cell_type":"code","source":"#training and test are split here\ntrainx, testx, trainy, testy = train_test_split(train_x, train_y, test_size=0.10, random_state=10)\n\ntrainx, valx, trainy, valy = train_test_split(trainx, trainy, test_size=0.15, random_state=10)\n\ntrainy = to_categorical(trainy)#categorizing the labels by converting the class labels to a binary class matrix\ntesty = to_categorical(testy)\nvaly = to_categorical(valy)\nnp.save('testx.npy', testx)\nnp.save('testy.npy', testy)\n\nprint(\"Training data number:\",len(trainx))\nprint(\"Testing data number:\",len(testx))\nprint(\"Validation data number:\",len(valx))\n\nprint(\"Training labels number:\",len(trainy))\nprint(\"Testing labels number:\",len(testy))\nprint(\"Validation labels number:\",len(valy))","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:34:05.775121Z","iopub.execute_input":"2021-10-26T12:34:05.775421Z","iopub.status.idle":"2021-10-26T12:34:05.950593Z","shell.execute_reply.started":"2021-10-26T12:34:05.775368Z","shell.execute_reply":"2021-10-26T12:34:05.949650Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Step 4 : Model Building","metadata":{}},{"cell_type":"code","source":"# model 1\n# The model is creating taking the base model as VGG19\nbase_model = VGG19(include_top = False, weights = 'imagenet', input_shape = (50,50,3), classes = trainy.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:34:44.230036Z","iopub.execute_input":"2021-10-26T12:34:44.230293Z","iopub.status.idle":"2021-10-26T12:34:47.429143Z","shell.execute_reply.started":"2021-10-26T12:34:44.230264Z","shell.execute_reply":"2021-10-26T12:34:47.428357Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"![VGG Network](https://www.researchgate.net/profile/Max-Ferguson/publication/322512435/figure/fig3/AS:697390994567179@1543282378794/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png)!","metadata":{}},{"cell_type":"code","source":"#Creating extra deep neural layers for transfer learning\nmodel= Sequential()\nmodel.add(base_model) \nmodel.add(Flatten()) ","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:36:17.580765Z","iopub.execute_input":"2021-10-26T12:36:17.581851Z","iopub.status.idle":"2021-10-26T12:36:17.653452Z","shell.execute_reply.started":"2021-10-26T12:36:17.581754Z","shell.execute_reply":"2021-10-26T12:36:17.652682Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:36:28.819675Z","iopub.execute_input":"2021-10-26T12:36:28.820248Z","iopub.status.idle":"2021-10-26T12:36:28.829109Z","shell.execute_reply.started":"2021-10-26T12:36:28.820209Z","shell.execute_reply":"2021-10-26T12:36:28.827655Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Adding the Dense layers along with activation and batch normalization\nmodel.add(Dense(1024,activation=('relu'),input_dim=512))\nmodel.add(Dense(512,activation=('relu'))) \nmodel.add(Dense(256,activation=('relu'))) \nmodel.add(Dropout(.3))\nmodel.add(Dense(128,activation=('relu')))\n#model.add(Dropout(.2))\nmodel.add(Dense(102,activation=('softmax'))) \n\n#Checking the final model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:36:31.038401Z","iopub.execute_input":"2021-10-26T12:36:31.038945Z","iopub.status.idle":"2021-10-26T12:36:31.098521Z","shell.execute_reply.started":"2021-10-26T12:36:31.038908Z","shell.execute_reply":"2021-10-26T12:36:31.097617Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#learning rate parameter to initialize the learning rate to the model\nlrr= ReduceLROnPlateau(monitor='val_acc', factor=.01, patience=3, min_lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:36:34.133673Z","iopub.execute_input":"2021-10-26T12:36:34.134358Z","iopub.status.idle":"2021-10-26T12:36:34.138497Z","shell.execute_reply.started":"2021-10-26T12:36:34.134321Z","shell.execute_reply":"2021-10-26T12:36:34.137428Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Initializing the hyperparameters\nbatch_size= 100\nepochs=50\nlearn_rate=.001\nsgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\nadam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:36:50.934124Z","iopub.execute_input":"2021-10-26T12:36:50.934387Z","iopub.status.idle":"2021-10-26T12:36:50.950930Z","shell.execute_reply.started":"2021-10-26T12:36:50.934357Z","shell.execute_reply":"2021-10-26T12:36:50.950162Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#fitting the first model\nmodel.fit(trainx, trainy, validation_data = (valx, valy), epochs=60, batch_size=20)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:36:53.513643Z","iopub.execute_input":"2021-10-26T12:36:53.514406Z","iopub.status.idle":"2021-10-26T12:46:24.792375Z","shell.execute_reply.started":"2021-10-26T12:36:53.514367Z","shell.execute_reply":"2021-10-26T12:46:24.791632Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Plotting the training and validation loss and accuracy\nf,ax=plt.subplots(2,1) \n\n#Loss\nax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\nax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n\n#Accuracy\nax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\nax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:46:27.928613Z","iopub.execute_input":"2021-10-26T12:46:27.929195Z","iopub.status.idle":"2021-10-26T12:46:28.219374Z","shell.execute_reply.started":"2021-10-26T12:46:27.929154Z","shell.execute_reply":"2021-10-26T12:46:28.218636Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model.save('vgg_transfer_learning_102_model.h5')","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:50:41.533466Z","iopub.execute_input":"2021-10-26T12:50:41.533735Z","iopub.status.idle":"2021-10-26T12:50:41.803835Z","shell.execute_reply.started":"2021-10-26T12:50:41.533705Z","shell.execute_reply":"2021-10-26T12:50:41.803092Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Here we get the training accuracy to be 99% but the validation accuracy is 79%(maximum).","metadata":{}},{"cell_type":"markdown","source":"## Step 5 : Testing the model","metadata":{}},{"cell_type":"code","source":"# Model 1 Testing\nfrom keras.models import load_model\nmodel = load_model(\"vgg_transfer_learning_102_model.h5\")\n\nscore = model.evaluate(testx, testy)\n\nprint('Test loss:', score[0]) \nprint('Test accuracy:', score[1])\n\n#Predict output on sample input data\npred = model.predict(testx) \npred = np.argmax(pred, axis = 1)[:10] \nlabel = np.argmax(testy,axis = 1)[:10] \n\nprint(\"Predicted labels:\",pred) \nprint(\"Actual Labels:   \",label)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T12:50:44.288709Z","iopub.execute_input":"2021-10-26T12:50:44.289306Z","iopub.status.idle":"2021-10-26T12:50:46.681035Z","shell.execute_reply.started":"2021-10-26T12:50:44.289268Z","shell.execute_reply":"2021-10-26T12:50:46.680182Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"### Hence the model seems to perform well with an accuracy of 74.23%. Further accuracy can be increased by introducing better architectures like MobileNet or ResNet or by using data augmentation technique.","metadata":{}},{"cell_type":"markdown","source":"# Refrences","metadata":{}},{"cell_type":"markdown","source":"1. https://analyticsindiamag.com/transfer-learning-for-multi-class-image-classification-using-deep-convolutional-neural-network/\n2. https://www.kaggle.com/nadeemsk4347/classifying-flowers-with-keras\n\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}